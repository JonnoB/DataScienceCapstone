---
title: "Untitled"
output: html_document
---

```{r}
options(java.parameters = "-Xmx2g")
packages <- c("R.utils", "tm", "RWeka", "dplyr", "stringr", "xtable", "ggplot2")
invisible(lapply(packages, require, character.only = TRUE))
```


```{r}
setwd("~/R/DataScienceCapstone")
base <- getwd()
setwd("./Data/Raw/final/en_US")
```


```{r}
dataSets <- data.frame(names = c("blogs","news", "twitter"),files = c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt"), stringsAsFactors=FALSE)
```

```{r}
for (i in 1:nrow(dataSets)){
(assign(paste(dataSets$names[i],"Raw", sep=""),readLines(dataSets$files[i]))) 
}

dataSets <- cbind(dataSets,
                  fileSize = sapply( dataSets$files, function(n) file.info(n)$size), 
                  objectSize = sapply( dataSets$names, 
                                       function(n) {
                                         object.size(eval(parse(
                                           text=paste(n, "Raw", sep="")
                                           )
                                           )
                                           )
                                         })
                                       , 
                  longestString = sapply(dataSets$names, 
                                         function(n) {
                                           max(nchar(eval(parse(
                                             text=paste(n,"Raw", sep="")))))}
                                         ))
```


```{r}
corpus_clean<- function(x) {
x <- tolower(x)
x <- gsub("&", "and", x)
x <- gsub("p m|p.m", "pm", x)
x <- gsub("'ll", " will", x)
x <- gsub("'d", " would", x)
x <- gsub("'m", " am", x)
x <- gsub("'ve", " have", x)
x <- gsub("'re", " are", x)
x <- gsub("can't|cannot|cant", "can not", x)
x <- gsub("n't", " not", x)
x <- gsub("'s", "", x)
x <- removeNumbers(x)
x <- removePunctuation(x)
x <- gsub(" will", "''ll", x)
x <- gsub(" would", "'d", x)
x <- gsub(" am", "'m", x)
x <- gsub(" have", "'ve", x)
x <- gsub(" are", "'re", x)
x <- gsub("cannot", "can't", x)
x <- gsub(" not", "n't", x)
x <- stripWhitespace(x)
  }

```

```{r}
corpus_clean_stop<- function(x) {
x <- tolower(x)
x <- gsub("&", "and", x)
x <- gsub("p m|p.m", "pm", x)
x <- gsub("'ll", " will", x)
x <- gsub("'d", " would", x)
x <- gsub("'m", " am", x)
x <- gsub("'ve", " have", x)
x <- gsub("'re", " are", x)
x <- gsub("can't|cannot|cant", "can not", x)
x <- gsub("n't", " not", x)
x <- gsub("'s", "", x)
x <- removeNumbers(x)
x <- removePunctuation(x)
x <- removeWords(x, stopwords("english"))
x <- stripWhitespace(x)
  }
```


Ngram function
```{r}
apply_ngram <- function(split_corpus, Ngram, wordFreq = 0) {

z2 <- lapply(split_corpus, function(n) { 
  x <- as.data.frame(NGramTokenizer(n, 
                   Weka_control(min = Ngram, max = Ngram, delimiters = " \\r\\n\\t.%*$,;#\\-:\"()?!")
            ), stringsAsFactors = F)
colnames(x) <- "pattern"
  x <- group_by(x, pattern ) %>%
  summarise(Freq = n())
        }
  )

xx <-  rbind_all(z2)
xx <- group_by(xx, pattern)
xx <- summarise(xx, Freq = sum(Freq))%>%
  filter( Freq >= wordFreq) %>%
  ungroup %>%
  arrange(-Freq)}
```


for very large Ngram sets
```{r}
large_Ngram2 <- function(dataset, Ngram ,start = 1, end= 8) {

  x <- lapply(dataset[start:end], function(i) {
          groups <- ceiling(length(i)/20000)
          group_split <- sample(1:groups, length(i), replace = TRUE)
          split_corpus <- lapply(1:groups, function(n) i[group_split == n])
          y <- apply_ngram(split_corpus, Ngram, wordFreq = 2)

          }
        )
  x <- rbind_all(x)
  x <- group_by(x, pattern) %>%
      summarise(Freq = sum(Freq))
  name <- deparse(substitute(dataset))
  y <- paste(name,"_",Ngram, "gram", sep="")
  assign(y, x, pos =1)  
  
}
```


create the probability table of the Ngrams
```{r}
Ngram_probs <- function(Ngram_main, Ngram_sub) {

                Ngram_main <- cbind(Ngram_main, 
                       dependent = word(Ngram_main$pattern, -1), 
                       predictor = word(Ngram_main$pattern, 1,-2), 
                       stringsAsFactors = FALSE)

                Ngram_main <- merge(Ngram_main, 
                          Ngram_sub, by.x ="predictor", by.y = "pattern") %>%
                          arrange(-Freq.x) %>%
                          rename( Freq= Freq.x, Prob = Freq.y, Ngrams = Ngrams.x ) %>%
                          select(-Ngrams.y) %>%
                          mutate(Prob = Freq/Prob)%>%
                          group_by(predictor) %>%
                          filter(min_rank(desc(Prob)) == 1) 
}
```


create the test set table this function is almost the same as the previous one except it doesn't filter out all the less likely combinations
```{r}
Ngram_test <- function(Ngram_main, Ngram_sub) {

                Ngram_main <- cbind(Ngram_main, 
                       dependent = word(Ngram_main$pattern, -1), 
                       predictor = word(Ngram_main$pattern, 1,-2), 
                       stringsAsFactors = FALSE)

                Ngram_main <- merge(Ngram_main, 
                          Ngram_sub, by.x ="predictor", by.y = "pattern") %>%
                          arrange(-Freq.x) %>%
                          rename( Freq= Freq.x, Prob = Freq.y, Ngrams = Ngrams.x ) %>%
                          select(-Ngrams.y) %>%
                          mutate(Prob = Freq/Prob)
}
```

clean the data sets
```{r}
news <- corpus_clean(newsRaw)
blogs <- corpus_clean(blogsRaw)
twitter <- corpus_clean(twitterRaw)
```

splitting up the datasets into 10% list form
```{r}
set.seed(1025)
twitter_groups <- sample(1:10, size = length(twitter),replace = TRUE)
news_groups <- sample(1:10, size = length(news),replace = TRUE)
blogs_groups <- sample(1:10, size = length(blogs),replace = TRUE)

twitter <- lapply(1:10, function(n) twitter[twitter_groups == n])
news <- lapply(1:10, function(n) news[news_groups == n])
blogs <- lapply(1:10, function(n) blogs[blogs_groups == n])

rm(twitter_groups,news_groups,blogs_groups)
```

Create Ngrams for the different sources
```{r}
large_Ngram2(twitter, 1, start = 1, end = 8)
large_Ngram2(twitter, 2, start = 1, end = 8)
large_Ngram2(twitter, 3, start = 1, end = 8)
large_Ngram2(twitter, 4, start = 1, end = 8)

large_Ngram2(blogs, 1, start = 1, end = 8)
large_Ngram2(blogs, 2, start = 1, end = 8)
large_Ngram2(blogs, 3, start = 1, end = 8)
large_Ngram2(blogs, 4, start = 1, end = 8)

large_Ngram2(news, 1, start = 1, end = 8)
large_Ngram2(news, 2, start = 1, end = 8)
large_Ngram2(news, 3, start = 1, end = 8)
large_Ngram2(news, 4, start = 1, end = 8)
```

compile Ngrams
```{r}
Ngram1 <- rbind(twitter_1gram, news_1gram, blogs_1gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 1)
Ngram2 <- rbind(twitter_2gram, news_2gram, blogs_2gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 2)
Ngram3 <- rbind(twitter_3gram, news_3gram, blogs_3gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 3)
Ngram4 <- rbind(twitter_4gram, news_4gram, blogs_4gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 4)

```


```{r}
ngram_probs <- lapply(2:4, function(n) Ngram_probs(
  eval(parse(text = paste("Ngram",n, sep=""))), 
  eval(parse(text = paste("Ngram",n-1, sep="")))
  ))
```

Create the test set
```{r}
large_Ngram2(news, 4, start = 9, end = 9)
large_Ngram2(twitter, 4, start = 9, end = 9)
large_Ngram2(blogs, 4, start = 9, end = 9)

large_Ngram2(news, 5, start = 9, end = 9)
large_Ngram2(twitter, 5, start = 9, end = 9)
large_Ngram2(blogs, 5, start = 9, end = 9)

Ngram4 <- rbind(twitter_4gram, news_4gram, blogs_4gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 4)

Ngram5 <- rbind(twitter_5gram, news_5gram, blogs_5gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 5)

Test <- Ngram_test(Ngram5, Ngram4)
save(Test, file ="Test.Rda")
```


```{r}
save(ngram_probs, file ="ngram_probs.Rda")
save(list=ls(pattern = "blogs_.*|twitter_.*|news_.*"), file = "data.Rda"  )
rm(list = ls(pattern = "blogs_|twitter_|news_"))
```


#The stopless version

```{r}
news_stop <- corpus_clean_stop(newsRaw)
blogs_stop <- corpus_clean_stop(blogsRaw)
twitter_stop <- corpus_clean_stop(twitterRaw)
```


```{r}
set.seed(1025)
twitter_groups <- sample(1:10, size = length(twitter_stop),replace = TRUE)
news_groups <- sample(1:10, size = length(news_stop),replace = TRUE)
blogs_groups <- sample(1:10, size = length(blogs_stop),replace = TRUE)

twitter_stop <- lapply(1:10, function(n) twitter_stop[twitter_groups == n])
news_stop <- lapply(1:10, function(n) news_stop[news_groups == n])
blogs_stop <- lapply(1:10, function(n) blogs_stop[blogs_groups == n])

rm(twitter_groups,news_groups,blogs_groups)
```

Create Ngrams for the different sources
```{r}
large_Ngram2(twitter_stop, 1, start = 1, end = 8)
large_Ngram2(twitter_stop, 2, start = 1, end = 8)
large_Ngram2(twitter_stop, 3, start = 1, end = 8)
large_Ngram2(twitter_stop, 4, start = 1, end = 8)

large_Ngram2(blogs_stop, 1, start = 1, end = 8)
large_Ngram2(blogs_stop, 2, start = 1, end = 8)
large_Ngram2(blogs_stop, 3, start = 1, end = 8)
large_Ngram2(blogs_stop, 4, start = 1, end = 8)

large_Ngram2(news_stop, 1, start = 1, end = 8)
large_Ngram2(news_stop, 2, start = 1, end = 8)
large_Ngram2(news_stop, 3, start = 1, end = 8)
large_Ngram2(news_stop, 4, start = 1, end = 8)
```

compile Ngrams
```{r}
Ngram1_stop <- rbind(twitter_stop_1gram, news_stop_1gram, blogs_stop_1gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 1)
Ngram2_stop <- rbind(twitter_stop_2gram, news_stop_2gram, blogs_stop_2gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 2)
Ngram3_stop <- rbind(twitter_stop_3gram, news_stop_3gram, blogs_stop_3gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 3)
Ngram4_stop <- rbind(twitter_stop_4gram, news_stop_4gram, blogs_stop_4gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 4)

```

```{r}
ngram_probs_stop <- lapply(2:4, function(n) Ngram_probs(
  eval(parse(text = paste("Ngram",n,"_stop",  sep=""))), 
  eval(parse(text = paste("Ngram",n-1, "_stop", sep="")))
  ))

```


```{r}
save(ngram_probs_stop, file ="ngram_probs_stop.Rda")
save(list=ls(pattern = "blogs_.*|twitter_.*|news_.*"), file = "data_stop.Rda"  )
rm(list = ls(pattern = "blogs_|twitter_|news_"))
```


creating the probability tables
```{r, eval=FALSE}
load("ngram_probs_stop.Rda")
load("ngram_probs.Rda")
```

##make the probs lists smaller

The Ngram lists are very large when first produced the full Nragm with stops is easily over 100Mb, in order to reduce the size to make searching and loading to shiny reasonable there needs to be a trade off between loss of information and size reduction.
```{r}
reduce <- lapply(ngram_probs, function(x){
  reduce <- sapply(1:30, function(n){
  x2 <-x[x$Freq >=n,]
  c(n,nrow(x2)/nrow(x),sum(x2$Freq)/sum(x$Freq))
})

data.frame(t(reduce))
})

red <- rbind_all(reduce)

red <- cbind( red, rep(c(2,3,4), each = 30))
names(red) <- c("cut_off","length_reduction", "Loss_of_information", "Ngram")
red <-melt(data = red, id.vars =c("cut_off", "Ngram"))

ggplot(red, aes( x= cut_off, y = value, colour = variable)) + geom_line()+facet_grid(~Ngram) +
  ggtitle ("Cut off points and associated row reduction and loss of information\n by Ngram size")

```


As a result of the above analysis and graph it seems reasonble to reduce the size of the ngram lists by a certain amount to make the search process quicker and also that the dictionaries can be laoded up to the shiny server.

the cuts off will be as follows

- Ngram2 cut off 30
- Ngram3 cut off 20
- Ngram4 cut off 10


```{r}
crush <- data.frame(1:3, c(30,20,10))
ngram_probs_small <- lapply(1:3, function(n) {
  filter(ngram_probs[[crush[n,1]]], Freq >=crush[n,2])
})
save(ngram_probs_small, file = "ngram_probs_small.Rda")
saveRDS(ngram_probs_small, file = "ngramsmall.RDS")
load("ngram_probs_small.Rda")
```


##The single most common word

```{r}
Ngram1 <- Ngram1 %>%
  arrange(desc(Freq)) %>%
  mutate( predictor = "[blank]" ,Prob = Freq/sum(Freq), dependent = pattern)

if_in_doubt <- Ngram1[1,]
```



##Creating and output

```{r}
result <- function(phrase, ngram_probs) { 
  phrase <- str_trim(phrase, side = "both")
  
  wordcount <- if(length(ngram_probs) < str_count(phrase, "\\S+")) {
  length(ngram_probs)
  } else  {str_count(phrase, "\\S+")}
  
  forNgrams <- sapply(1:wordcount, function(n) word(phrase, -n, -1))
  results <-lapply(1:wordcount, function(n) filter(ngram_probs[[n]], 
                                                    predictor == forNgrams[[n]]))
results <- do.call(rbind.data.frame, results)
results <- ungroup(results) %>%
  arrange( desc(Prob), desc(Ngrams))
  results
  }
```

```{r}
result_test <- function(phrase, ngram_probs) { 
  phrase <- str_trim(phrase, side = "both")
  wordcount <- length(ngram_probs)
  
  forNgrams <- sapply(1:wordcount, function(n) word(phrase, -n, -1))
  results <-lapply(1:wordcount, function(n) filter(ngram_probs[[n]], 
                                                    predictor == forNgrams[[n]]))
results <- rbind_all(results)
results <- ungroup(results) %>%
  arrange( desc(Prob), desc(Ngrams))
  results[1,5]
  }
```


testing the Ngram model
```{r} 
pattern <- "yum enjoy your"

phrase <- corpus_clean(pattern)
phrase_stop <- corpus_clean_stop(pattern)

    results <- rbind(result_blank(phrase, ngram_probs_blank), result(phrase, ngram_probs)) %>%
      arrange( desc(Ngrams), desc(Prob))

```


testing a sample of Ngrams to see the accuracy
```{r}
pred_acc <- sample_n(Test, size = 100, replace = TRUE, weight = Freq)

y <- sapply(pred_acc$predictor, function(n) {
  
  result_test(n, ngram_probs2)
})
pred_acc <- cbind(pred_acc, result = do.call(rbind, y))

with(pred_acc, sum((dependent == result)*Freq, na.rm = TRUE)/sum(Freq))

save(pred_acc, file = "predictive_accuracy.Rda")

sum(pred_acc$Freq)/sum(Test$Freq) #percent of total tested
```


The dplyr enhanced version that avoids the table function is faster. it is also much smaller as it doesn't have a huge list of factors only characters
```{r}
# twit <- twitter[1:2000]
# group_split <- sample(1:10, length(twit), replace = TRUE)
# split_corpus <- lapply(1:10, function(n) twit[group_split == n])
# 
# res <- microbenchmark(apply_ngram2(split_corpus, 2),apply_ngram(split_corpus, 2), times =1000)
# save(res, file= "apply_ngram_bench.Rda")
```

replaced the origina apply_ngram
```{r}
# apply_ngram2 <- function(split_corpus, Ngram, wordFreq = 20) {
# 
# z2 <- lapply(split_corpus, function(n) { 
#   x <- as.data.frame(NGramTokenizer(n, 
#                    Weka_control(min = Ngram, max = Ngram, delimiters = " \\r\\n\\t.%*$,;#'\\-:\"()?!")
#             ), stringsAsFactors = F)
# colnames(x) <- "pattern"
#   x <- group_by(x, pattern ) %>%
#   summarise(Freq = n())
#         }
#   )
# 
# xx <-  rbind_all(z2)
# xx <- group_by(xx, pattern)
# xx <- summarise(xx, Freq = sum(Freq))%>%
#   filter( Freq > wordFreq) %>%
#   ungroup %>%
#   arrange(-Freq)}
```

```{r}

#old out classed by the newer benchmark function
# apply_ngram <- function(split_corpus, Ngram, wordFreq = 20) {
# 
# z2 <- lapply(split_corpus, function(n) { 
#   data.frame(table(NGramTokenizer(n, 
#                    Weka_control(min = Ngram, max = Ngram, delimiters = " \\r\\n\\t.,;:\"()?!")
#                    )
#         ),
#        stringsAsFactors = FALSE )
#         }
#   )
# 
# xx <- do.call(rbind.data.frame, z2)
# xx <- group_by(xx, Var1)
# xx <- summarise(xx, Freq = sum(Freq))%>%
#   filter( Freq > wordFreq) %>%
#   ungroup %>%
#   arrange(-Freq)}
```

This code chunk is only useful when the large Ngram function is not being used as the large Ngram function has this chunk built in.
```{r, eval=FALSE}
# groups <- ceiling(length(sub_corpus)/20000)
# group_split <- sample(1:groups, length(sub_corpus), replace = TRUE)
# split_corpus <- lapply(1:groups, function(n) sub_corpus[group_split == n])
```


application of the apply ngram function out side of large_nrgam
```{r, eval=FALSE}
# Ngram4 <- apply_ngram(split_corpus, 4)
# Ngram3 <- apply_ngram(split_corpus, 3)
# Ngram2 <- apply_ngram(split_corpus, 2)
# Ngram1 <- apply_ngram(split_corpus, 1)
```