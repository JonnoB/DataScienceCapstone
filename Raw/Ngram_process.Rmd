---
title: "Making Ngrams dictionairies for Word prediction"
author: "Jonathan Bourne"
date: "Friday, December 12, 2014"
output: html_document
---

##Setup

```{r}
options(java.parameters = "-Xmx2g")
packages <- c("R.utils", "tm", "RWeka", "dplyr", "stringr", "xtable", "ggplot2", "reshape2")
invisible(lapply(packages, require, character.only = TRUE))
```


```{r}
setwd("~/R/DataScienceCapstone")
base <- getwd()
setwd("./Data/Raw/final/en_US")
```


```{r}
dataSets <- data.frame(names = c("blogs","news", "twitter"),files = c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt"), stringsAsFactors=FALSE)
```

```{r}
for (i in 1:nrow(dataSets)){
(assign(dataSets$names[i],readLines(dataSets$files[i]))) 
}

dataSets <- cbind(dataSets,
                  fileSize = sapply( dataSets$files, function(n) file.info(n)$size), 
                  objectSize = sapply( dataSets$names, 
                                       function(n) {
                                         object.size(eval(parse(
                                           text=paste(n, "Raw", sep="")
                                           )
                                           )
                                           )
                                         })
                                       , 
                  longestString = sapply(dataSets$names, 
                                         function(n) {
                                           max(nchar(eval(parse(
                                             text=paste(n,"Raw", sep="")))))}
                                         ))
```

#Creating Ngram Probability tables

##Functions required to build the Ngram Probability tables


Corpus cleaninging function
```{r}
corpus_clean<- function(x) {
x <- tolower(x)
x <- gsub("&", "and", x)
x <- gsub("p m|p.m", "pm", x)
x <- gsub("'ll", " will", x)
x <- gsub("'d", " would", x)
x <- gsub("'m", " am", x)
x <- gsub("'ve", " have", x)
x <- gsub("'re", " are", x)
x <- gsub("can't|cannot|cant", "can not", x)
x <- gsub("n't", " not", x)
x <- gsub("'s", "", x)
x <- removeNumbers(x)
x <- removePunctuation(x)
x <- gsub(" will", "''ll", x)
x <- gsub(" would", "'d", x)
x <- gsub(" am", "'m", x)
x <- gsub(" have", "'ve", x)
x <- gsub(" are", "'re", x)
x <- gsub("cannot", "can't", x)
x <- gsub(" not", "n't", x)
x <- stripWhitespace(x)
  }

```

Ngram function
```{r}
apply_ngram <- function(split_corpus, Ngram, wordFreq = 0) {

z2 <- lapply(split_corpus, function(n) { 
  x <- as.data.frame(NGramTokenizer(n, 
                   Weka_control(min = Ngram, max = Ngram, delimiters = " \\r\\n\\t.%*$,;#\\-:\"()?!")
            ), stringsAsFactors = F)
colnames(x) <- "pattern"
  x <- group_by(x, pattern ) %>%
  summarise(Freq = n())
        }
  )

xx <-  rbind_all(z2)
xx <- group_by(xx, pattern)
xx <- summarise(xx, Freq = sum(Freq))%>%
  filter( Freq >= wordFreq) %>%
  ungroup %>%
  arrange(-Freq)}
```


for very large Ngram sets
```{r}
large_Ngram2 <- function(dataset, Ngram ,start = 1, end= 8) {

  x <- lapply(dataset[start:end], function(i) {
          groups <- ceiling(length(i)/20000)
          group_split <- sample(1:groups, length(i), replace = TRUE)
          split_corpus <- lapply(1:groups, function(n) i[group_split == n])
          y <- apply_ngram(split_corpus, Ngram, wordFreq = 2)

          }
        )
  x <- rbind_all(x)
  x <- group_by(x, pattern) %>%
      summarise(Freq = sum(Freq))
  name <- deparse(substitute(dataset))
  y <- paste(name,"_",Ngram, "gram", sep="")
  assign(y, x, pos =1)  
  
}
```


create the probability table of the Ngrams
```{r}
Ngram_probs <- function(Ngram_main, Ngram_sub) {

                Ngram_main <- cbind(Ngram_main, 
                       dependent = word(Ngram_main$pattern, -1), 
                       predictor = word(Ngram_main$pattern, 1,-2), 
                       stringsAsFactors = FALSE)

                Ngram_main <- merge(Ngram_main, 
                          Ngram_sub, by.x ="predictor", by.y = "pattern") %>%
                          arrange(-Freq.x) %>%
                          rename( Freq= Freq.x, Prob = Freq.y, Ngrams = Ngrams.x ) %>%
                          select(-Ngrams.y) %>%
                          mutate(Prob = Freq/Prob)%>%
                          group_by(predictor) %>%
                          filter(min_rank(desc(Prob)) == 1) 
}
```


create the test set table this function is almost the same as the previous one except it doesn't filter out all the less likely combinations
```{r}
Ngram_test <- function(Ngram_main, Ngram_sub) {

                Ngram_main <- cbind(Ngram_main, 
                       dependent = word(Ngram_main$pattern, -1), 
                       predictor = word(Ngram_main$pattern, 1,-2), 
                       stringsAsFactors = FALSE)

                Ngram_main <- merge(Ngram_main, 
                          Ngram_sub, by.x ="predictor", by.y = "pattern") %>%
                          arrange(-Freq.x) %>%
                          rename( Freq= Freq.x, Prob = Freq.y, Ngrams = Ngrams.x ) %>%
                          select(-Ngrams.y) %>%
                          mutate(Prob = Freq/Prob)
}
```


##Processing data to make Ngram Probability tables

clean the data sets
```{r}
news <- corpus_clean(newsRaw)
blogs <- corpus_clean(blogsRaw)
twitter <- corpus_clean(twitterRaw)
```

splitting up the datasets into 10% list form
```{r}
set.seed(1025)
twitter_groups <- sample(1:10, size = length(twitter),replace = TRUE)
news_groups <- sample(1:10, size = length(news),replace = TRUE)
blogs_groups <- sample(1:10, size = length(blogs),replace = TRUE)

twitter <- lapply(1:10, function(n) twitter[twitter_groups == n])
news <- lapply(1:10, function(n) news[news_groups == n])
blogs <- lapply(1:10, function(n) blogs[blogs_groups == n])

rm(twitter_groups,news_groups,blogs_groups)
```

##Create the Ngram dictionaries

Create Ngrams for the different sources

```{r}
large_Ngram2(twitter, 1, start = 1, end = 8)
large_Ngram2(twitter, 2, start = 1, end = 8)
large_Ngram2(twitter, 3, start = 1, end = 8)
large_Ngram2(twitter, 4, start = 1, end = 8)

large_Ngram2(blogs, 1, start = 1, end = 8)
large_Ngram2(blogs, 2, start = 1, end = 8)
large_Ngram2(blogs, 3, start = 1, end = 8)
large_Ngram2(blogs, 4, start = 1, end = 8)

large_Ngram2(news, 1, start = 1, end = 8)
large_Ngram2(news, 2, start = 1, end = 8)
large_Ngram2(news, 3, start = 1, end = 8)
large_Ngram2(news, 4, start = 1, end = 8)
```

compile Ngrams
```{r}
Ngram1 <- rbind(twitter_1gram, news_1gram, blogs_1gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 1)
Ngram2 <- rbind(twitter_2gram, news_2gram, blogs_2gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 2)
Ngram3 <- rbind(twitter_3gram, news_3gram, blogs_3gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 3)
Ngram4 <- rbind(twitter_4gram, news_4gram, blogs_4gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 4)

```

##Create the List of Ngrams

```{r}
ngram_probs <- lapply(2:4, function(n) Ngram_probs(
  eval(parse(text = paste("Ngram",n, sep=""))), 
  eval(parse(text = paste("Ngram",n-1, sep="")))
  ))
```

##Create the test set

In order to see how well a test set functions it is necessary to have a hold out test set that wasn't part of the training set. For this the 9th group of the original data set has been chosen. this subset will be treated the same as the training set and compiled into Ngrams. however it will only be compiled into 5grams as this can test 4-3-2 and 1grams and uses less time to create and test. 

```{r}
large_Ngram2(news, 4, start = 9, end = 9)
large_Ngram2(twitter, 4, start = 9, end = 9)
large_Ngram2(blogs, 4, start = 9, end = 9)

large_Ngram2(news, 5, start = 9, end = 9)
large_Ngram2(twitter, 5, start = 9, end = 9)
large_Ngram2(blogs, 5, start = 9, end = 9)

Ngram4 <- rbind(twitter_4gram, news_4gram, blogs_4gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 4)

Ngram5 <- rbind(twitter_5gram, news_5gram, blogs_5gram)%>%
    group_by( pattern) %>%
    summarise(Freq = sum(Freq), Ngrams = 5)

Test <- Ngram_test(Ngram5, Ngram4)
```


```{r}
save(ngram_probs, file ="ngram_probs.Rda")
save(Test, file ="Test.Rda")
save(list=ls(pattern = "blogs_.*|twitter_.*|news_.*"), file = "data.Rda"  )
rm(list = ls(pattern = "blogs_|twitter_|news_"))
```


#Creating unknown word prediction 

The Ngram lists that have been created are fine at prediction the next word when they have seen all the words in a phrase however if a word in a phrase has not been seen they cannot predict. The intial way around this problem is to use a shorter Ngram model go from say 4 to 3 to 2, however if the last word in the phrase is unknown this technique will not help. an example of this would be

- I want to drive a car
- I want to drive a Ferrari
- I want to drive a Krank600

It is possible that the first two phrases are known to the model however it is highly unlikey that the last one is known. This means that as the last word in the phrase in unknown the model will not be able to predict anything at all. In order to avoid this the model needs to be able to ignore certain unknown words and use only the precedeeing words this would make the last phrase become

- I want to drive a [blank]

which the model has seen several times before. This approach allows the model to focus on the structure of the sentence as opposed to the the words int eh sentence.

The skip over structure that has been chosen is as follows

- word [blank] word
- word [blank] [blank]
- [blank]

THe code used to make such Ngram dictionaries is shown below.


Function to replace words with "[blank]" where required.
```{r}
insert_blank <- function(x , words, row = 1){

  t <- lapply(1:Ngram_case, function(n) {
  if(words[row,n] =="[blank]"){rep("[blank]", nrow(x) )} else{word(x$predictor, n)}
  })

x$predictor <- do.call(paste, t)
x$pattern <- paste(x$predictor ,x$dependent)

x <- group_by(x, pattern)
x <- summarise(x, predictor = predictor, 
               Freq = sum(Freq), 
               Ngrams = Ngram_case - sum(grepl("[blank]",words[row,]))+1, 
               dependent = dependent)%>%
      ungroup  %>%
      arrange(-Freq)

x <- group_by(x, predictor ) %>%
      mutate( Prob = Freq/sum(Freq)) %>%
      filter(min_rank(desc(Prob)) == 1) }
```

##The single most common word

In case that all words entered are entirely unknown it is useful to have the most commonly used word as a back up, sounds a bit like a cheat, but you always have to suggest something.
```{r}
Ngram1 <- Ngram1 %>%
  arrange(desc(Freq)) %>%
  mutate( predictor = "[blank]" ,Prob = Freq/sum(Freq), dependent = pattern)

if_in_doubt <- Ngram1[1,] #unsurprisingly the most common word is "the" further testing to see whether it is the most common word after an unknown word would be a sensible idea, however due to time contraints this will not be done.
```


##Blank spaces Ngrams

A dataframe of possible word/blank structures. There are many  different combinations of blanks word structures for the larger Ngrams, extensive testing of different combinations of Ngram blank structures would be needed to find the optimal configurations whilst minimising size. However in this case only two simple structures will be used. If required however this table structure and the insert_blank function can be combined to create them all.
```{r}
words <- data.frame(one = rep("x1",8), two = rep(c("x2","[blank]"), 1, each = 4), three = rep(c("x3","[blank]"), 2, each = 2), four = rep(c("x4","[blank]"), 4), stringsAsFactors = FALSE)
```


```{r}

Ngram4 <- ngram_probs[[3]]
Ngram_case <- mean(Ngram4$Ngrams)-1
words2 <- words[!duplicated(words[,1:Ngram_case]),1:Ngram_case]
Ngram4_blank <- insert_blank(Ngram4, words2, 4)
Ngram4_blank$predictor <- word(Ngram4_blank$predictor, 1) 
Ngram4_blank$Ngrams <- 1.5

Ngram3 <- ngram_probs[[3]]
Ngram_case <- mean(Ngram3$Ngrams)-1
words2 <- words[!duplicated(words[,1:Ngram_case]),1:Ngram_case]
Ngram3_blank <- insert_blank(Ngram3, words2, 3)
Ngram3_blank$predictor <- word(Ngram3_blank$predictor, 1)
Ngram3_blank$Ngrams <- 2.5

ngram_probs_blank <- list(if_in_doubt,Ngram3_blank, Ngram4_blank)
save(ngram_probs_blank, file = "ngram_probs_blank.Rda")
```

#Compressing the dictionaries

The Ngram lists are very large when first produced the full Nragm_probs list is easily over 100Mb, in order to reduce the size to make searching and loading to shiny reasonable there needs to be a trade off between loss of information and size reduction.


function that shows how data can be reduced compressed for a certain loss of data
```{r}
reducer <- function(Ngram, max_cut_off = 50) {
 
  reduce <- sapply(1:max_cut_off, function(n){
  Ngram2 <-Ngram[Ngram$Freq >=n,]
  c(n,nrow(Ngram2)/nrow(Ngram),sum(Ngram2$Freq)/sum(Ngram$Freq))
})  
reduce <- data.frame(t(reduce)) 
names(reduce) <- c("cut_off","length_reduction", "Loss_of_information")  
reduce <-melt(data = reduce, id.vars ="cut_off")
}
```


##Compressing the main dictionary

```{r}

reduce <- lapply(ngram_probs, function(x){
  reducer(x, 30)
 })
                 
reduce <- cbind( rbind_all(reduce), Ngram= rep(c(2,3,4), each = 60))

ggplot(reduce, aes( x= cut_off, y = value, colour = variable)) + geom_line()+facet_grid(~Ngram) +
  ggtitle ("Cut off points and associated row reduction and loss of information\n by Ngram size")
ggsave(file="reduce.png")

reduce_ratio <- dcast(reduce, cut_off + Ngram ~variable) 
reduce_ratio <- mutate(reduce_ratio_blank, Ngram = as.factor(Ngram), ratio = length_reduction/Loss_of_information)

ggplot(reduce_ratio, aes( x = cut_off, y= ratio, colour = Ngram)) + geom_line()+ 
  ggtitle("Ratio of total total length_reduction/Loss_of_information by cut off")+
  xlab("cut off")

```


As a result of the above analysis and graph it seems reasonble to reduce the size of the ngram lists by a certain amount to make the search process quicker and also that the dictionaries can be laoded up to the shiny server.

the cuts off will be as follows

- Ngram2 cut off 30
- Ngram3 cut off 20
- Ngram4 cut off 10


```{r}
crush <- data.frame(1:3, c(30,20,10))
ngram_probs_small <- lapply(1:3, function(n) {
  filter(ngram_probs[[crush[n,1]]], Freq >=crush[n,2])
})
save(ngram_probs_small, file = "ngram_probs_small.Rda")
```


##Compressing the blanks dictionary

The blank dictionaires are considerably smaller as by removing variability from the data set a lot of information can be stored in a smaller space at a small cost to information density. however the same compression test will be run to see how much compression can take place.

```{r}

reduce_blank <- lapply(ngram_probs_blank, function(x){
  reducer(x, 30)
 })
                 
reduce_blank <- cbind( rbind_all(reduce_blank), Ngram= rep(c(1,2.5,1.5), each = 60))

ggplot(reduce_blank, aes( x= cut_off, y = value, colour = variable)) + geom_line()+facet_grid(~Ngram) +
  ggtitle ("Cut off points and associated row reduction and loss of information\n by Ngram size")

reduce_ratio_blank <- dcast(reduce_blank, cut_off + Ngram ~variable) 
reduce_ratio_blank <- mutate(reduce_ratio_blank, Ngram = as.factor(Ngram), ratio = length_reduction/Loss_of_information)

ggplot(reduce_ratio_blank, aes( x = cut_off, y= ratio, colour = Ngram)) + geom_line()+ 
  ggtitle("Ratio of total total length_reduction/Loss_of_information by cut off")+
  xlab("cut off")
```


The analysis shows that there can be some substatial compression with very little loss of information, however as the blanks dictionary is quite small anyway this compression doesn't have to be too aggressive.

the cut offs will be as follows

- Ngram1 cut off is irrelevant as there is only 1
- Ngram1.5 cut off 5
- Ngram2.5 cut off 5


```{r}
crush <- data.frame(1:3, c(1,5,10))
ngram_probs_blank_small <- lapply(1:3, function(n) {
  filter(ngram_probs_blank[[crush[n,1]]], Freq >=crush[n,2])
})
save(ngram_probs_blank_small, file = "ngram_probs_blank_small.Rda")
```


After compression the dictionaries have been reduced to approximately 10% of there original size.

#Creating an output

The Ngrams lists have to actually be able to output a single value which will be the reccomended word. the below functions look at how to achieve this and how the predictive power of the dictionaries can be tested.

```{r}
result <- function(phrase, ngram_probs) { 
  phrase <- str_trim(phrase, side = "both")
  
  wordcount <- if(length(ngram_probs) < str_count(phrase, "\\S+")) {
  length(ngram_probs)
  } else  {str_count(phrase, "\\S+")}
  
  forNgrams <- sapply(1:wordcount, function(n) word(phrase, -n, -1))
  results <-lapply(1:wordcount, function(n) filter(ngram_probs[[n]], 
                                                    predictor == forNgrams[[n]]))
results <- do.call(rbind.data.frame, results)
results <- ungroup(results) %>%
  arrange( desc(Prob), desc(Ngrams))
  results
  }
```

```{r}
result_blank <- function(phrase, ngram_probs) { 
  phrase <- str_trim(phrase, side = "both")
  
  wordcount <- if(length(ngram_probs) < str_count(phrase, "\\S+")) {
  length(ngram_probs)
  } else  {str_count(phrase, "\\S+")}
  
  forNgrams <- sapply(1:wordcount, function(n) word(phrase, -n))
  results <-lapply(1:wordcount, function(n) filter(ngram_probs[[n]], 
                                                    predictor == forNgrams[[n]] | predictor == "[blank]"))
  results <- rbind_all(results)
  results <- ungroup(results) %>%
  arrange( desc(Prob), desc(Ngrams))
  results
  }
```

##Testing on lots of phrases
This test function is a slightly faster version of the regular results function designed to test a data frame of phrases

```{r}
result_test <- function(phrase, ngram_probs) { 
  phrase <- str_trim(phrase, side = "both")
  wordcount <- length(ngram_probs)
  
  forNgrams <- sapply(1:wordcount, function(n) word(phrase, -n, -1))
  results <-lapply(1:wordcount, function(n) filter(ngram_probs[[n]], 
                                                    predictor == forNgrams[[n]]))
results <- rbind_all(results)
results <- ungroup(results) %>%
  arrange( desc(Prob), desc(Ngrams))
  }
```


```{r}
result_blank_test <- function(phrase, ngram_probs) { 
  phrase <- str_trim(phrase, side = "both")
  
  wordcount <- length(ngram_probs)
  
  forNgrams <- sapply(1:wordcount, function(n) word(phrase, -n))
  results <-lapply(1:wordcount, function(n) filter(ngram_probs[[n]], 
                                                    predictor == forNgrams[[n]] | predictor == "[blank]"))
  results <- rbind_all(results)
  results <- ungroup(results) %>%
  arrange( desc(Prob), desc(Ngrams))
  results
  }
```

##Predicting

##Predicting a single phrase
The below code is to directly solve the given task. That is to predict the subsequent word in an incomplete phrase.

```{r} 
pattern <- "yum enjoy your"

phrase <- corpus_clean(pattern)

    results <- rbind(result_blank_test(phrase, ngram_probs_blank), result_test(phrase, ngram_probs)) %>%
      arrange( desc(Ngrams), desc(Prob))
results$dependent[1]

    results2 <- rbind(result_blank(phrase, ngram_probs_blank_small), result(phrase, ngram_probs_small)) %>%
      arrange( desc(Ngrams), desc(Prob))

```

##Predicitng model accuracy

```{r}
samples = 5000
pred_acc <- sample_n(Test, size = samples, replace = TRUE, weight = Freq)

test <- lapply(pred_acc$predictor, function(n) {
  
    results <- rbind(result_blank_test(n, ngram_probs_blank), 
                     result_test(n, ngram_probs)) %>%
      arrange( desc(Ngrams), desc(Prob))
results$dependent[1]
})

test_small <- lapply(pred_acc$predictor, function(n) {
  
    results <- rbind(result_blank_test(n, ngram_probs_blank_small), 
                     result_test(n, ngram_probs_small)) %>%
      arrange( desc(Ngrams), desc(Prob))
results$dependent[1]
})

pred_acc <- cbind(pred_acc, result = do.call(rbind, small),result_small = do.call(rbind, small_test))

small_acc <- with(pred_acc, sum((dependent == result_small), na.rm = TRUE)/nrow(pred_acc))
acc <- with(pred_acc, sum((dependent == result), na.rm = TRUE)/nrow(pred_acc))

size.reduction <-(object.size(ngram_probs_small)+ object.size(ngram_probs_blank_small))/(object.size(ngram_probs)+ object.size(ngram_probs_blank))

size.reduction <- as.numeric(size.reduction)
save(size.reduction, file = "size_reduction.Rda")
save(pred_acc, file = "predictive_accuracy.Rda")

acc_diff <- data.frame( percCorrect= colSums(matrix((pred_acc$dependent == pred_acc$result), nrow = 100)), dictionary = "Full")
acc_diff <- rbind(acc_diff, data.frame( percCorrect= colSums(matrix((pred_acc$dependent == pred_acc$result_small), nrow = 100)), dictionary = "Small"))
)

ggplot(data = acc_diff, aes( x = dictionary, y = percCorrect, fill = dictionary)) +geom_boxplot()+ggtitle("DIfference in accuracy between the normal and small dictionaries\n bootstrapped on 50 averages of 100 repititions")
ggsave("acc_diff.png")

ggplot(acc_diff, aes(x= percCorrect, colour = dictionary)) + geom_density()

test_diff <- with(acc_diff, t.test(percCorrect[dictionary == "Full"], 
               percCorrect[dictionary == "Small"]))

mean_full <- mean(acc_diff$percCorrect[acc_diff$dictionary == "Full"])
mean_small <- mean(acc_diff$percCorrect[acc_diff$dictionary == "Small"])
```


#Conclusions

Testing the predictive accuracy of the model over `r samples` samples showed that using the base dictionary and the base blanks dictionary gives a predictive power of `r acc` compared to using the compressed versions of the dictionaries which gives a predictive power of `r small_acc` The difference was significant at the p0.95 confidence level. This change of `r 1- small_acc/acc` comes with a size saving of `r 1-size.reduction`. As a result the smaller dictionaries will be used in the final shiny app.