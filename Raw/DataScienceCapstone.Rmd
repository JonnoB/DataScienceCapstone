---
title: "NLP coursework"
author: "Jonathan Bourne"
date: "Saturday, November 08, 2014"
output: html_document
---

```{r}
options(java.parameters = "-Xmx2g")
packages <- c("R.utils", "tm", "RWeka", "dplyr", "stringr", "xtable")
invisible(lapply(packages, require, character.only = TRUE))
```


```{r}
setwd("~/R/DataScienceCapstone")
base <- getwd()
setwd("./Data/Raw/final/en_US")
```


things to consider removing all "-", expanding "n't" from both the data set and the data entry part.

sub "can't" for "cannot"

then "n't" for " not".... another alternactive is to remove the negation entirely from the the text as it only reverses the meaning of the sentence nothing else


```{r}
dataSets <- data.frame(names = c("blogs","news", "twitter"),files = c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt"), stringsAsFactors=FALSE)

#dataSets <- cbind(dataSets, totalLines = sapply(dataSets$files, countLines))
```

import datasets and check file size
```{r}
for (i in 1:nrow(dataSets)){
(assign(dataSets$names[i],readLines(dataSets$files[i]))) #can lapply be used here?
}

dataSets <- cbind(dataSets,
                  fileSize = sapply( dataSets$files, function(n) file.info(n)$size), 
                  objectSize = sapply( dataSets$names, 
                                       function(n) object.size(eval(parse(text=n)))), 
                  longestString = sapply(dataSets$names, 
                                         function(n) max(nchar(eval(parse(text=n))))))
```



create sub samples and combined sub corpus
```{r}
sample <- sapply(dataSets$names, function(n) rbinom(length(eval(parse(text=n))), 1, 0.3))

for ( i in 1:nrow(dataSets)){
  n <- eval(parse(text=dataSets$names[i]))[as.logical(sample[[i]])]
  assign(paste(dataSets$names[i], "_sub", sep=""),n )

}

sub_corpus <- tolower(c(blogs_sub, news_sub, twitter_sub))
```

Quiz questions
```{r}
# Q4
# love <-sum(grepl("love", blogs, ignore.case=TRUE))
# hate <-sum(grepl("hate", blogs, ignore.case=TRUE))
# love.case1 <-sum(grepl("love", blogs))
# love.case2 <-sum(grepl("(?i)love", blogs))
# love/hate
# 
# Q5
# twitter[grep("biostats", twitter)]
# 
# 
# Q6
# unlikely <- "A computer once beat me at chess, but it was no match for me at kickboxing"
# twitter[grep(unlikely, twitter)]
```

work tokenising
```{r}
# wordtokens <- function(corpus) {
#   data.frame((table(WordTokenizer(tolower(corpus), control = NULL)))) %>%
#   arrange( -Freq) %>%
#   mutate(percent = Freq/sum(Freq), cumPercent = cumsum(percent) )
#   }

# tokens_blogs <- wordtokens(blogs_sub)
# tokens_news <- wordtokens(news_sub)
# tokens_twitter <- wordtokens(twitter_sub)
```

Ngram making
```{r}
Ngramiffier <- function(corpus, Ngram = 2 ) {
  as.data.frame(table(
    NGramTokenizer(corpus, 
                   Weka_control(min = Ngram, max = Ngram, delimiters = " \\r\\n\\t.,;:\"()?!")
                   )
    ),
              stringsAsFactors = FALSE) %>%
  arrange( -Freq) %>%
  mutate(percent = Freq/sum(Freq), cumPercent = cumsum(percent), Ngrams = Ngram )
  }

corpus_Ngram5 <- Ngramiffier(sub_corpus, Ngram = 5)
corpus_Ngram4 <- Ngramiffier(sub_corpus, Ngram = 4)
corpus_Ngram3 <- Ngramiffier(sub_corpus, Ngram = 3)
corpus_Ngram2 <- Ngramiffier(sub_corpus, Ngram = 2)
corpus_Ngram1 <- Ngramiffier(sub_corpus, Ngram = 1)

corpus_names <- c("corpus_Ngram5","corpus_Ngram4", "corpus_Ngram3", "corpus_Ngram2", "corpus_Ngram1" )

```



```{r}
# plot(corpus_Ngram2$cumPercent, corpus_Ngram2$Freq)
# plot(corpus_Ngram2$cumPercent[1:60000] )
# plot(corpus_Ngram2$cumPercent)
# 
# length(table(corpus_Ngram4$Freq))
# plot((table(corpus_Ngram4$Freq)/length(corpus_Ngram4$Freq)))
# 
# rareCombos <- sapply(corpus_names ,function(n) sum(table(eval(parse(text=n))$Freq)[1:2]/length(eval(parse(text=n))$Freq)))
# 
# barplot(rareCombos) #the quantity of the ngram counts that are made up of single or double observations

Ngram_probs <- function(Ngram_main, Ngram_sub, Freq_greater_than =20) {
                Ngram_main <- filter(Ngram_main, Freq >Freq_greater_than)
                Ngram_main <- cbind(Ngram_main, 
                       dependent = word(Ngram_main$Var1, -1), 
                       predictor = word(Ngram_main$Var1, 1,-2), 
                       stringsAsFactors = FALSE)

                Ngram_main <- merge(Ngram_main, 
                           select(Ngram_sub, Var1, Freq), by.x ="predictor", by.y = "Var1") %>%
                arrange(-Freq.x) %>%
                rename( Freq= Freq.x, Prob = Freq.y ) %>%
                mutate(Prob = Freq/Prob)%>%
                group_by(predictor) %>%
                filter(min_rank(desc(Prob)) == 1) 
}


ngram_probs <- lapply(2:5, function(n) Ngram_probs(
  eval(parse(text = paste("corpus_Ngram",n, sep=""))), 
  eval(parse(text = paste("corpus_Ngram",n-1, sep=""))), 2))

pattern <- "and a case of"

wordcount <- str_count(pattern, "\\S+")

forNgrams <- sapply(1:wordcount, function(n) word(pattern, -n, -1))

results <-lapply(1:wordcount, function(n) filter(ngram_probs[[n]], predictor == forNgrams[[n]]))
results <- do.call(rbind.data.frame, results)
results <- ungroup(results) %>%
  arrange( desc(Prob), desc(Ngrams))


```


Questions to consider

What do the data look like? 
  blocks of text randomly taken from the interenet
Where do the data come from? 
  either bogs, news reports or twitter
Can you think of any other data sources that might help you in this project?
  Project Guttenberg could be a source for open available text, although it might include unsual language
What are the common steps in natural language processing?
I dunno!
What are some common issues in the analysis of text data?
Profantiy filtering
What is the relationship between NLP and the concepts you have learned in the Specialization?

```{r}
# library(tm)
# library("RWeka")
# library(RWekajars)
# NGramTokenizer(source1, Weka_control(min = 1, max = 1))
# ```
# 
# 
# ```{r}
# library(RWeka)
# 
#  txt <- "I don't know. Maybe they're getting too much sun. I think I'm going to cut them way back. I replied."
# NGramTokenizer(txt, Weka_control(min = 1, max = 1, delimiters = " \\r\\n\\t.,;:\"()?!"))
# AlphabeticTokenizer(news_sub)
# table(WordTokenizer(txt, control = NULL))
# 
# news2 <- readLines("en_US.news2.txt")
# news2_sub <- news2[as.logical(sample[[2]])]
# news3_sub <- news[as.logical(rbinom(length(news), 1, 0.01))]
# 
# tokens_news3 <- data.frame((table(WordTokenizer(tolower(news3_sub), control = NULL)))) %>%
#   arrange( -Freq) %>%
#   mutate(percent = Freq/sum(Freq), cumPercent = cumsum(percent) )
# 
# 
# tokens_news <- data.frame((table(WordTokenizer(tolower(news_sub), control = NULL)))) %>%
#   arrange( -Freq) %>%
#   mutate(percent = Freq/sum(Freq), cumPercent = cumsum(percent) )
# 
# tokens_blogs <- data.frame((table(WordTokenizer(tolower(blogs_sub), control = NULL)))) %>%
#   arrange( -Freq) %>%
#   mutate(percent = Freq/sum(Freq), cumPercent = cumsum(percent) )
# 
# tokens_twitter <- data.frame((table(WordTokenizer(tolower(twitter_sub), control = NULL)))) %>%
#   arrange( -Freq) %>%
#   mutate(percent = Freq/sum(Freq), cumPercent = cumsum(percent) )
# 
# wordtokens <- function(corpus) {
#   data.frame((table(WordTokenizer(tolower(corpus), control = NULL)))) %>%
#   arrange( -Freq) %>%
#   mutate(percent = Freq/sum(Freq), cumPercent = cumsum(percent) )
#   }
# tokens_blogs <- wordtokens(blogs_sub)
# tokens_news <- wordtokens(news_sub)
# tokens_twitter <- wordtokens(twitter_sub)
# 
# Ngramtoken <- function(corpus) {
#   data.frame((table(NGramTokenizer(corpus, Weka_control(min = 3, max = 3, delimiters = " \\r\\n\\t.,;:\"()?!"))))) %>%
#   arrange( -Freq) %>%
#   mutate(percent = Freq/sum(Freq), cumPercent = cumsum(percent) )
#   }
#   
#   ngram_news <- Ngramtoken(twitter_sub)
```